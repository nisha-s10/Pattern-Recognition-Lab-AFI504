# ğŸ§ª Experiment 3: Maximum Likelihood (MLE) and Maximum A Posteriori (MAP) Estimation for Gaussian Models

## ğŸ“Œ Overview

This project demonstrates the implementation of:

- Maximum Likelihood Estimation (MLE)
- Maximum A Posteriori (MAP) Estimation

for Gaussian models using a real-world dataset.

All parameter estimation is implemented **from scratch**, without using built-in estimation functions.

---

## ğŸ“Š Dataset Used

Wine Dataset  
Source: `sklearn.datasets.load_wine`

- Total Samples: 178
- Features: 13 continuous features
- Classes: 3 (used only for dataset context)
- This experiment focuses on parameter estimation, not classification.

---

## ğŸ¯ Objective

To estimate the mean and variance parameters of a Gaussian distribution using:

- Maximum Likelihood Estimation (MLE)
- Maximum A Posteriori Estimation (MAP)

and analyze the effect of prior information on parameter estimation.

---

## ğŸ§  Theory

### Gaussian Distribution

For a Gaussian random variable:

X ~ N(Î¼, ÏƒÂ²)

---

### ğŸ”¹ Maximum Likelihood Estimation (MLE)

MLE estimates parameters by maximizing the likelihood function:

$$
\hat{\mu}_{MLE} = \frac{1}{N} \sum_{i=1}^{N} x_i
$$

$$
\hat{\sigma}_{MLE}^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{\mu})^2
$$

MLE depends only on observed data.

---

### ğŸ”¹ Maximum A Posteriori (MAP)

MAP incorporates prior belief about parameters.

Assume prior:

Î¼ ~ N(Î¼â‚€, Ï„Â²)

Then the MAP estimate becomes:

$$
\hat{\mu}_{MAP} =
\frac{\frac{N}{\sigma^2}\bar{x} + \frac{1}{\tau^2}\mu_0}
{\frac{N}{\sigma^2} + \frac{1}{\tau^2}}
$$

MAP shrinks the estimate toward the prior mean.

---

## âš™ï¸ Implementation Details

Steps performed:

1. Load Wine dataset
2. Select one feature for Gaussian modeling
3. Standardize data
4. Compute MLE mean and variance
5. Compute MAP mean using Gaussian prior
6. Compare estimates for:
   - Full dataset
   - Small sample subset
7. Visualize:
   - Histogram of data
   - Gaussian curves (MLE vs MAP)
   - Log-likelihood function
   - Log-posterior function
   - Effect of prior strength

All computations are performed manually using NumPy.

---

## ğŸ“ˆ Visualizations Included

- Histogram of standardized feature
- Gaussian curve (MLE)
- Gaussian curve (MAP)
- Log-Likelihood vs Mean plot
- Log-Posterior vs Mean plot
- Small sample MLE vs MAP comparison
- Prior variance impact visualization

---

## ğŸ“Š Key Observations

- For large sample sizes:
  MAP â‰ˆ MLE (likelihood dominates prior)

- For small sample sizes:
  MAP shifts toward prior mean (shrinkage effect)

- As N â†’ âˆ:
  MAP â†’ MLE

This demonstrates the theoretical relationship between likelihood and posterior estimation.

---

## ğŸ§® Concepts Demonstrated

- Likelihood maximization
- Bayesian parameter estimation
- Effect of prior distribution
- Shrinkage behavior
- Bias-variance tradeoff
- Log-likelihood and log-posterior optimization

---

## ğŸ›  Technologies Used

- Python
- NumPy
- Matplotlib
- Scikit-learn (dataset loading only)

---

## ğŸš€ How to Run

1. Clone the repository
2. Install required packages:

pip install numpy matplotlib scikit-learn

3. Run the notebook or Python script.

---

## ğŸ“ Academic Context

This experiment is part of a Pattern Recognition lab and focuses on understanding:

- Parameter estimation techniques
- Differences between frequentist and Bayesian approaches
- Practical behavior of MLE and MAP estimators

---

## ğŸ‘©â€ğŸ’» Author

Nisha Singh  
B.Tech â€“ Pattern Recognition Lab  

---

## ğŸ“œ License

This project is intended for academic and educational use.
